# -*- coding: utf-8 -*-
"""RL final model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r2JhYoCTNtbs8gRCnuEpsDjbw4c_gdjq

# Imports
"""

import gym
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.patches as mpatches
from pathlib import Path
import IPython.display as display

"""# Initializations and functions"""

# Define data directory
DATA_DIR = Path("/saved_data/")
if not DATA_DIR.exists():
    DATA_DIR.mkdir()

# Define a function to discretize the state space
def discretize_state(state, num_states):
    pos_bins = np.linspace(-1.2, 0.6, num_states[0])
    vel_bins = np.linspace(-0.07, 0.07, num_states[1])
    pos_digitized = np.digitize(state[0], pos_bins)
    vel_digitized = np.digitize(state[1], vel_bins)
    return pos_digitized - 1, vel_digitized - 1  # Adjusting to start from index 0

# Define a function to save Q-table and other information
def save_data(q_table, episode, rewards, steps):
    data = {
        "q_table": q_table,
        "episode": episode,
        "rewards": rewards,
        "steps": steps
    }
    np.save(DATA_DIR / "saved_data.npy", data)

# Define a function to load Q-table and other information
def load_data():
    data_file = DATA_DIR / "saved_data.npy"
    if data_file.exists():
        data = np.load(data_file, allow_pickle=True).item()
        return data["q_table"], data["episode"], data["rewards"], data["steps"]
    else:
        return None, 0, [], []

# Define a function to run Q-learning on the MountainCar-v0 environment
def run_q_learning(episodes, q_table=None, learning_rate=0.1, discount_factor=0.95, epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.995):
    # Initialize the environment
    env = gym.make('MountainCar-v0')
    num_actions = env.action_space.n
    num_states = (20, 20)  # discretized state space
    max_position = None

    # Initialize Q-table if not provided
    if q_table is None:
        q_table = np.zeros((num_states[0], num_states[1], num_actions))

    # Load data if available
    q_table_saved, episode_saved, rewards_saved, steps_saved = load_data()
    if q_table_saved is not None:
        q_table = q_table_saved

    # Initialize variables for tracking rewards and steps per episode
    rewards_per_episode = []
    steps_per_episode = []
    successes = 0
    # Main Q-learning loop
    for episode in range(episode_saved, episode_saved + episodes):
        if episode % 100 == 0:
          print("Episode: ", episode, "Successes: ", successes)
        state = env.reset()  # reset environment to beginning for each episode
        total_reward = 0
        done = False
        steps = 0

        while not done:
            # Discretize the state
            state_discrete = discretize_state(state, num_states)

            # Choose action using epsilon-greedy policy
            if np.random.rand() < epsilon_start:
                action = env.action_space.sample()  # take random action
            else:
                action = np.argmax(q_table[state_discrete[0], state_discrete[1]])

            # Take action and observe next state and reward
            next_state, reward, done, _ = env.step(action)

            current_position = next_state[0]
            if max_position:
              max_position = max(current_position, max_position)
            else:
              max_position = current_position
            # Discretize the next state
            next_state_discrete = discretize_state(next_state, num_states)

            # Update Q-table using Q-learning equation
            q_table[state_discrete[0], state_discrete[1], action] += learning_rate * (
                    reward + discount_factor * np.max(q_table[next_state_discrete[0], next_state_discrete[1]]) - q_table[state_discrete[0], state_discrete[1], action])

            total_reward += reward
            state = next_state
            steps += 1

        # Decay epsilon
        epsilon_start = epsilon_start - 2/episodes if epsilon_start>0.01 else 0.01

        # Append rewards and steps per episode
        rewards_per_episode.append(total_reward)
        steps_per_episode.append(steps)
        if max_position >= 0.5:
              successes += 1

        # Save data after each episode
        save_data(q_table, episode + 1, rewards_per_episode, steps_per_episode)

    env.close()
    return rewards_per_episode, steps_per_episode, q_table, successes

# Define a function to test Q-table
def test_q_table(q_table, epsilon):
    env = gym.make('MountainCar-v0')
    state = env.reset()
    done = False
    total_reward = 0
    img = plt.imshow(env.render('rgb_array')) # only call this once
    successes = 0
    while not done:
        state_discrete = discretize_state(state, (20, 20))
        action = np.argmax(q_table[state_discrete[0], state_discrete[1]])
        next_state, reward, done, _ = env.step(action)

        if next_state[0]>= 0.5:
          successes += 1
        total_reward += reward
        state = next_state
        #env.render()
        img.set_data(env.render('rgb_array')) # just update the data
        display.display(plt.gcf())
        display.clear_output(wait=True)
        action = env.action_space.sample()
        if np.random.rand() < epsilon:
            action = env.action_space.sample()  # take random action
        else:
            action = np.argmax(q_table[state_discrete[0], state_discrete[1]])
        env.step(action)

    print("Total Reward:", total_reward)
    print("Success:", successes)
    env.close()
    return successes

# Define a function to test Q-table
def test_q_table_no_display(q_table, epsilon):
    env = gym.make('MountainCar-v0')
    state = env.reset()
    done = False
    total_reward = 0
    #img = plt.imshow(env.render('rgb_array')) # only call this once
    successes = 0
    while not done:
        state_discrete = discretize_state(state, (20, 20))
        action = np.argmax(q_table[state_discrete[0], state_discrete[1]])
        next_state, reward, done, _ = env.step(action)

        if next_state[0]>= 0.5:
          successes += 1
        total_reward += reward
        state = next_state
        #env.render()
        #img.set_data(env.render('rgb_array')) # just update the data
        #display.display(plt.gcf())
        #display.clear_output(wait=True)
        action = env.action_space.sample()
        if np.random.rand() < epsilon:
            action = env.action_space.sample()  # take random action
        else:
            action = np.argmax(q_table[state_discrete[0], state_discrete[1]])
        env.step(action)

    print("Total Reward:", total_reward)
    print("Success:", successes)
    env.close()
    return successes

"""# Training"""

episodes = 20000
rewards, steps, q_table, successes = run_q_learning(episodes)

"""# For understanding and parameter tuning"""

max(rewards), successes

q_table_saved, episode_saved, rewards_saved, steps_saved = load_data()
if q_table_saved is not None:
    q_table = q_table_saved

f = open("test_scores.txt","a")
for e in [0]: #[0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1, 0]:
  s = 0
  for i in range(1000):
    s += test_q_table_no_display(q_table, e)
  print("epsilon: ", e, "Average successes: ", s/10)
  f.write(f"epsilon: {e}, Average successes: {s/10} \n")
f.close()

"""# Testing"""

q_table_saved, episode_saved, rewards_saved, steps_saved = load_data()
if q_table_saved is not None:
    q_table = q_table_saved

s = test_q_table(q_table, 0)

s

episode_saved # Number of episodes used for training

